{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 204.54545454545453,
  "eval_steps": 500,
  "global_step": 4500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 3.6809890270233154,
      "learning_rate": 0.001993939393939394,
      "loss": 2.4039,
      "step": 20
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 3.3403513431549072,
      "learning_rate": 0.001987878787878788,
      "loss": 1.6985,
      "step": 40
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 3.1459145545959473,
      "learning_rate": 0.001981818181818182,
      "loss": 1.5257,
      "step": 60
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 3.451941967010498,
      "learning_rate": 0.001975757575757576,
      "loss": 1.4909,
      "step": 80
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 5.089279651641846,
      "learning_rate": 0.00196969696969697,
      "loss": 1.3534,
      "step": 100
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 8.238877296447754,
      "learning_rate": 0.0019636363636363636,
      "loss": 1.3051,
      "step": 120
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 4.890087604522705,
      "learning_rate": 0.0019575757575757574,
      "loss": 1.2957,
      "step": 140
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 5.327608585357666,
      "learning_rate": 0.0019515151515151514,
      "loss": 1.2114,
      "step": 160
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 7.067874908447266,
      "learning_rate": 0.0019454545454545456,
      "loss": 1.133,
      "step": 180
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 5.137452602386475,
      "learning_rate": 0.0019393939393939396,
      "loss": 1.125,
      "step": 200
    },
    {
      "epoch": 10.0,
      "grad_norm": 7.329897880554199,
      "learning_rate": 0.0019333333333333333,
      "loss": 1.1187,
      "step": 220
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 10.206101417541504,
      "learning_rate": 0.0019272727272727273,
      "loss": 1.0061,
      "step": 240
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 6.480746269226074,
      "learning_rate": 0.0019212121212121211,
      "loss": 1.0095,
      "step": 260
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 8.440860748291016,
      "learning_rate": 0.0019151515151515151,
      "loss": 0.9575,
      "step": 280
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 8.546219825744629,
      "learning_rate": 0.0019090909090909091,
      "loss": 0.9514,
      "step": 300
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 9.718132019042969,
      "learning_rate": 0.001903030303030303,
      "loss": 0.8738,
      "step": 320
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 6.9331374168396,
      "learning_rate": 0.001896969696969697,
      "loss": 0.8801,
      "step": 340
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 14.422216415405273,
      "learning_rate": 0.0018909090909090909,
      "loss": 0.6767,
      "step": 360
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 8.03908920288086,
      "learning_rate": 0.0018848484848484849,
      "loss": 0.6733,
      "step": 380
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 7.548478603363037,
      "learning_rate": 0.0018787878787878789,
      "loss": 0.6859,
      "step": 400
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 6.593507766723633,
      "learning_rate": 0.0018727272727272729,
      "loss": 0.6485,
      "step": 420
    },
    {
      "epoch": 20.0,
      "grad_norm": 8.612811088562012,
      "learning_rate": 0.0018666666666666666,
      "loss": 0.6104,
      "step": 440
    },
    {
      "epoch": 20.90909090909091,
      "grad_norm": 12.928461074829102,
      "learning_rate": 0.0018606060606060606,
      "loss": 0.5341,
      "step": 460
    },
    {
      "epoch": 21.818181818181817,
      "grad_norm": 9.55541706085205,
      "learning_rate": 0.0018545454545454546,
      "loss": 0.5402,
      "step": 480
    },
    {
      "epoch": 22.727272727272727,
      "grad_norm": 9.423993110656738,
      "learning_rate": 0.0018484848484848484,
      "loss": 0.5531,
      "step": 500
    },
    {
      "epoch": 23.636363636363637,
      "grad_norm": 10.439866065979004,
      "learning_rate": 0.0018424242424242426,
      "loss": 0.5466,
      "step": 520
    },
    {
      "epoch": 24.545454545454547,
      "grad_norm": 13.9585542678833,
      "learning_rate": 0.0018363636363636364,
      "loss": 0.5256,
      "step": 540
    },
    {
      "epoch": 25.454545454545453,
      "grad_norm": 5.776402950286865,
      "learning_rate": 0.0018303030303030304,
      "loss": 0.4465,
      "step": 560
    },
    {
      "epoch": 26.363636363636363,
      "grad_norm": 6.850262641906738,
      "learning_rate": 0.0018242424242424242,
      "loss": 0.4645,
      "step": 580
    },
    {
      "epoch": 27.272727272727273,
      "grad_norm": 12.224916458129883,
      "learning_rate": 0.0018181818181818182,
      "loss": 0.3949,
      "step": 600
    },
    {
      "epoch": 28.181818181818183,
      "grad_norm": 6.086468696594238,
      "learning_rate": 0.001812121212121212,
      "loss": 0.3918,
      "step": 620
    },
    {
      "epoch": 29.09090909090909,
      "grad_norm": 11.253473281860352,
      "learning_rate": 0.0018060606060606062,
      "loss": 0.4238,
      "step": 640
    },
    {
      "epoch": 30.0,
      "grad_norm": 34.47303009033203,
      "learning_rate": 0.0018000000000000002,
      "loss": 0.3568,
      "step": 660
    },
    {
      "epoch": 30.90909090909091,
      "grad_norm": 6.622166633605957,
      "learning_rate": 0.001793939393939394,
      "loss": 0.3396,
      "step": 680
    },
    {
      "epoch": 31.818181818181817,
      "grad_norm": 7.401883602142334,
      "learning_rate": 0.001787878787878788,
      "loss": 0.3527,
      "step": 700
    },
    {
      "epoch": 32.72727272727273,
      "grad_norm": 11.25735092163086,
      "learning_rate": 0.0017818181818181817,
      "loss": 0.3263,
      "step": 720
    },
    {
      "epoch": 33.63636363636363,
      "grad_norm": 6.4555816650390625,
      "learning_rate": 0.001775757575757576,
      "loss": 0.2837,
      "step": 740
    },
    {
      "epoch": 34.54545454545455,
      "grad_norm": 4.622404098510742,
      "learning_rate": 0.0017696969696969697,
      "loss": 0.3304,
      "step": 760
    },
    {
      "epoch": 35.45454545454545,
      "grad_norm": 8.0280122756958,
      "learning_rate": 0.0017636363636363637,
      "loss": 0.2721,
      "step": 780
    },
    {
      "epoch": 36.36363636363637,
      "grad_norm": 8.716614723205566,
      "learning_rate": 0.0017575757575757577,
      "loss": 0.2646,
      "step": 800
    },
    {
      "epoch": 37.27272727272727,
      "grad_norm": 4.4526472091674805,
      "learning_rate": 0.0017515151515151515,
      "loss": 0.2484,
      "step": 820
    },
    {
      "epoch": 38.18181818181818,
      "grad_norm": 4.01416540145874,
      "learning_rate": 0.0017454545454545455,
      "loss": 0.2593,
      "step": 840
    },
    {
      "epoch": 39.09090909090909,
      "grad_norm": 4.074315071105957,
      "learning_rate": 0.0017393939393939395,
      "loss": 0.2837,
      "step": 860
    },
    {
      "epoch": 40.0,
      "grad_norm": 17.849092483520508,
      "learning_rate": 0.0017333333333333335,
      "loss": 0.2685,
      "step": 880
    },
    {
      "epoch": 40.90909090909091,
      "grad_norm": 7.620932102203369,
      "learning_rate": 0.0017272727272727272,
      "loss": 0.2393,
      "step": 900
    },
    {
      "epoch": 41.81818181818182,
      "grad_norm": 15.953546524047852,
      "learning_rate": 0.0017212121212121212,
      "loss": 0.2353,
      "step": 920
    },
    {
      "epoch": 42.72727272727273,
      "grad_norm": 13.950396537780762,
      "learning_rate": 0.001715151515151515,
      "loss": 0.2284,
      "step": 940
    },
    {
      "epoch": 43.63636363636363,
      "grad_norm": 10.831616401672363,
      "learning_rate": 0.001709090909090909,
      "loss": 0.1937,
      "step": 960
    },
    {
      "epoch": 44.54545454545455,
      "grad_norm": 8.59550952911377,
      "learning_rate": 0.0017030303030303032,
      "loss": 0.2493,
      "step": 980
    },
    {
      "epoch": 45.45454545454545,
      "grad_norm": 4.324997901916504,
      "learning_rate": 0.001696969696969697,
      "loss": 0.2409,
      "step": 1000
    },
    {
      "epoch": 46.36363636363637,
      "grad_norm": 5.062279224395752,
      "learning_rate": 0.001690909090909091,
      "loss": 0.1896,
      "step": 1020
    },
    {
      "epoch": 47.27272727272727,
      "grad_norm": 4.982816219329834,
      "learning_rate": 0.0016848484848484848,
      "loss": 0.1616,
      "step": 1040
    },
    {
      "epoch": 48.18181818181818,
      "grad_norm": 5.564700126647949,
      "learning_rate": 0.0016787878787878788,
      "loss": 0.1886,
      "step": 1060
    },
    {
      "epoch": 49.09090909090909,
      "grad_norm": 2.0756287574768066,
      "learning_rate": 0.0016727272727272726,
      "loss": 0.1865,
      "step": 1080
    },
    {
      "epoch": 50.0,
      "grad_norm": 7.788402557373047,
      "learning_rate": 0.0016666666666666668,
      "loss": 0.1864,
      "step": 1100
    },
    {
      "epoch": 50.90909090909091,
      "grad_norm": 5.7642645835876465,
      "learning_rate": 0.0016606060606060608,
      "loss": 0.1491,
      "step": 1120
    },
    {
      "epoch": 51.81818181818182,
      "grad_norm": 7.877990245819092,
      "learning_rate": 0.0016545454545454545,
      "loss": 0.1579,
      "step": 1140
    },
    {
      "epoch": 52.72727272727273,
      "grad_norm": 15.592632293701172,
      "learning_rate": 0.0016484848484848485,
      "loss": 0.2278,
      "step": 1160
    },
    {
      "epoch": 53.63636363636363,
      "grad_norm": 8.648114204406738,
      "learning_rate": 0.0016424242424242423,
      "loss": 0.1487,
      "step": 1180
    },
    {
      "epoch": 54.54545454545455,
      "grad_norm": 3.924628257751465,
      "learning_rate": 0.0016363636363636365,
      "loss": 0.2525,
      "step": 1200
    },
    {
      "epoch": 55.45454545454545,
      "grad_norm": 10.325887680053711,
      "learning_rate": 0.0016303030303030303,
      "loss": 0.1408,
      "step": 1220
    },
    {
      "epoch": 56.36363636363637,
      "grad_norm": 8.241209030151367,
      "learning_rate": 0.0016242424242424243,
      "loss": 0.1456,
      "step": 1240
    },
    {
      "epoch": 57.27272727272727,
      "grad_norm": 2.8551316261291504,
      "learning_rate": 0.0016181818181818183,
      "loss": 0.145,
      "step": 1260
    },
    {
      "epoch": 58.18181818181818,
      "grad_norm": 8.045464515686035,
      "learning_rate": 0.001612121212121212,
      "loss": 0.136,
      "step": 1280
    },
    {
      "epoch": 59.09090909090909,
      "grad_norm": 1.1395374536514282,
      "learning_rate": 0.001606060606060606,
      "loss": 0.148,
      "step": 1300
    },
    {
      "epoch": 60.0,
      "grad_norm": 10.536487579345703,
      "learning_rate": 0.0016,
      "loss": 0.1576,
      "step": 1320
    },
    {
      "epoch": 60.90909090909091,
      "grad_norm": 5.208315849304199,
      "learning_rate": 0.001593939393939394,
      "loss": 0.1436,
      "step": 1340
    },
    {
      "epoch": 61.81818181818182,
      "grad_norm": 4.839507579803467,
      "learning_rate": 0.0015878787878787879,
      "loss": 0.1103,
      "step": 1360
    },
    {
      "epoch": 62.72727272727273,
      "grad_norm": 4.622390270233154,
      "learning_rate": 0.0015818181818181818,
      "loss": 0.1389,
      "step": 1380
    },
    {
      "epoch": 63.63636363636363,
      "grad_norm": 6.712686538696289,
      "learning_rate": 0.0015757575757575756,
      "loss": 0.1228,
      "step": 1400
    },
    {
      "epoch": 64.54545454545455,
      "grad_norm": 9.21593189239502,
      "learning_rate": 0.0015696969696969696,
      "loss": 0.135,
      "step": 1420
    },
    {
      "epoch": 65.45454545454545,
      "grad_norm": 5.395198345184326,
      "learning_rate": 0.0015636363636363638,
      "loss": 0.1193,
      "step": 1440
    },
    {
      "epoch": 66.36363636363636,
      "grad_norm": 10.69874382019043,
      "learning_rate": 0.0015575757575757576,
      "loss": 0.1278,
      "step": 1460
    },
    {
      "epoch": 67.27272727272727,
      "grad_norm": 2.8394269943237305,
      "learning_rate": 0.0015515151515151516,
      "loss": 0.0734,
      "step": 1480
    },
    {
      "epoch": 68.18181818181819,
      "grad_norm": 6.735462665557861,
      "learning_rate": 0.0015454545454545454,
      "loss": 0.1295,
      "step": 1500
    },
    {
      "epoch": 69.0909090909091,
      "grad_norm": 6.110723972320557,
      "learning_rate": 0.0015393939393939394,
      "loss": 0.1185,
      "step": 1520
    },
    {
      "epoch": 70.0,
      "grad_norm": 3.3943822383880615,
      "learning_rate": 0.0015333333333333334,
      "loss": 0.0984,
      "step": 1540
    },
    {
      "epoch": 70.9090909090909,
      "grad_norm": 5.760441303253174,
      "learning_rate": 0.0015272727272727274,
      "loss": 0.0898,
      "step": 1560
    },
    {
      "epoch": 71.81818181818181,
      "grad_norm": 6.759546756744385,
      "learning_rate": 0.0015212121212121214,
      "loss": 0.0955,
      "step": 1580
    },
    {
      "epoch": 72.72727272727273,
      "grad_norm": 5.079279899597168,
      "learning_rate": 0.0015151515151515152,
      "loss": 0.0804,
      "step": 1600
    },
    {
      "epoch": 73.63636363636364,
      "grad_norm": 7.009016990661621,
      "learning_rate": 0.0015090909090909091,
      "loss": 0.1028,
      "step": 1620
    },
    {
      "epoch": 74.54545454545455,
      "grad_norm": 11.033567428588867,
      "learning_rate": 0.001503030303030303,
      "loss": 0.1074,
      "step": 1640
    },
    {
      "epoch": 75.45454545454545,
      "grad_norm": 7.106110572814941,
      "learning_rate": 0.0014969696969696971,
      "loss": 0.1145,
      "step": 1660
    },
    {
      "epoch": 76.36363636363636,
      "grad_norm": 2.052928924560547,
      "learning_rate": 0.001490909090909091,
      "loss": 0.0682,
      "step": 1680
    },
    {
      "epoch": 77.27272727272727,
      "grad_norm": 5.9024457931518555,
      "learning_rate": 0.001484848484848485,
      "loss": 0.1307,
      "step": 1700
    },
    {
      "epoch": 78.18181818181819,
      "grad_norm": 2.9825520515441895,
      "learning_rate": 0.0014787878787878787,
      "loss": 0.0808,
      "step": 1720
    },
    {
      "epoch": 79.0909090909091,
      "grad_norm": 2.4405717849731445,
      "learning_rate": 0.0014727272727272727,
      "loss": 0.0706,
      "step": 1740
    },
    {
      "epoch": 80.0,
      "grad_norm": 0.9283339977264404,
      "learning_rate": 0.0014666666666666667,
      "loss": 0.0912,
      "step": 1760
    },
    {
      "epoch": 80.9090909090909,
      "grad_norm": 7.792433261871338,
      "learning_rate": 0.0014606060606060607,
      "loss": 0.1273,
      "step": 1780
    },
    {
      "epoch": 81.81818181818181,
      "grad_norm": 4.168776512145996,
      "learning_rate": 0.0014545454545454547,
      "loss": 0.1003,
      "step": 1800
    },
    {
      "epoch": 82.72727272727273,
      "grad_norm": 9.86623764038086,
      "learning_rate": 0.0014484848484848485,
      "loss": 0.0768,
      "step": 1820
    },
    {
      "epoch": 83.63636363636364,
      "grad_norm": 6.427927017211914,
      "learning_rate": 0.0014424242424242424,
      "loss": 0.1044,
      "step": 1840
    },
    {
      "epoch": 84.54545454545455,
      "grad_norm": 3.9983978271484375,
      "learning_rate": 0.0014363636363636362,
      "loss": 0.0811,
      "step": 1860
    },
    {
      "epoch": 85.45454545454545,
      "grad_norm": 9.911781311035156,
      "learning_rate": 0.0014303030303030304,
      "loss": 0.098,
      "step": 1880
    },
    {
      "epoch": 86.36363636363636,
      "grad_norm": 7.610753536224365,
      "learning_rate": 0.0014242424242424244,
      "loss": 0.1121,
      "step": 1900
    },
    {
      "epoch": 87.27272727272727,
      "grad_norm": 3.122666597366333,
      "learning_rate": 0.0014181818181818182,
      "loss": 0.0635,
      "step": 1920
    },
    {
      "epoch": 88.18181818181819,
      "grad_norm": 6.895333766937256,
      "learning_rate": 0.0014121212121212122,
      "loss": 0.0777,
      "step": 1940
    },
    {
      "epoch": 89.0909090909091,
      "grad_norm": 3.541689395904541,
      "learning_rate": 0.001406060606060606,
      "loss": 0.0738,
      "step": 1960
    },
    {
      "epoch": 90.0,
      "grad_norm": 8.097532272338867,
      "learning_rate": 0.0014,
      "loss": 0.0758,
      "step": 1980
    },
    {
      "epoch": 90.9090909090909,
      "grad_norm": 4.690712928771973,
      "learning_rate": 0.001393939393939394,
      "loss": 0.0633,
      "step": 2000
    },
    {
      "epoch": 91.81818181818181,
      "grad_norm": 2.6464614868164062,
      "learning_rate": 0.001387878787878788,
      "loss": 0.0622,
      "step": 2020
    },
    {
      "epoch": 92.72727272727273,
      "grad_norm": 9.877396583557129,
      "learning_rate": 0.0013818181818181818,
      "loss": 0.0702,
      "step": 2040
    },
    {
      "epoch": 93.63636363636364,
      "grad_norm": 8.752167701721191,
      "learning_rate": 0.0013757575757575758,
      "loss": 0.0897,
      "step": 2060
    },
    {
      "epoch": 94.54545454545455,
      "grad_norm": 0.9959043860435486,
      "learning_rate": 0.0013696969696969697,
      "loss": 0.098,
      "step": 2080
    },
    {
      "epoch": 95.45454545454545,
      "grad_norm": 0.757651686668396,
      "learning_rate": 0.0013636363636363635,
      "loss": 0.0714,
      "step": 2100
    },
    {
      "epoch": 96.36363636363636,
      "grad_norm": 7.25286865234375,
      "learning_rate": 0.0013575757575757577,
      "loss": 0.0637,
      "step": 2120
    },
    {
      "epoch": 97.27272727272727,
      "grad_norm": 2.3733010292053223,
      "learning_rate": 0.0013515151515151515,
      "loss": 0.0502,
      "step": 2140
    },
    {
      "epoch": 98.18181818181819,
      "grad_norm": 4.369857311248779,
      "learning_rate": 0.0013454545454545455,
      "loss": 0.0503,
      "step": 2160
    },
    {
      "epoch": 99.0909090909091,
      "grad_norm": 4.150824546813965,
      "learning_rate": 0.0013393939393939393,
      "loss": 0.0473,
      "step": 2180
    },
    {
      "epoch": 100.0,
      "grad_norm": 3.8656466007232666,
      "learning_rate": 0.0013333333333333333,
      "loss": 0.0631,
      "step": 2200
    },
    {
      "epoch": 100.9090909090909,
      "grad_norm": 5.754384994506836,
      "learning_rate": 0.0013272727272727275,
      "loss": 0.0675,
      "step": 2220
    },
    {
      "epoch": 101.81818181818181,
      "grad_norm": 4.810503005981445,
      "learning_rate": 0.0013212121212121213,
      "loss": 0.0446,
      "step": 2240
    },
    {
      "epoch": 102.72727272727273,
      "grad_norm": 1.720205545425415,
      "learning_rate": 0.0013151515151515153,
      "loss": 0.0619,
      "step": 2260
    },
    {
      "epoch": 103.63636363636364,
      "grad_norm": 1.7665799856185913,
      "learning_rate": 0.001309090909090909,
      "loss": 0.0424,
      "step": 2280
    },
    {
      "epoch": 104.54545454545455,
      "grad_norm": 2.2817459106445312,
      "learning_rate": 0.001303030303030303,
      "loss": 0.0384,
      "step": 2300
    },
    {
      "epoch": 105.45454545454545,
      "grad_norm": 6.066411018371582,
      "learning_rate": 0.0012969696969696968,
      "loss": 0.027,
      "step": 2320
    },
    {
      "epoch": 106.36363636363636,
      "grad_norm": 2.0931625366210938,
      "learning_rate": 0.001290909090909091,
      "loss": 0.047,
      "step": 2340
    },
    {
      "epoch": 107.27272727272727,
      "grad_norm": 3.4049313068389893,
      "learning_rate": 0.001284848484848485,
      "loss": 0.0506,
      "step": 2360
    },
    {
      "epoch": 108.18181818181819,
      "grad_norm": 1.0282137393951416,
      "learning_rate": 0.0012787878787878788,
      "loss": 0.0421,
      "step": 2380
    },
    {
      "epoch": 109.0909090909091,
      "grad_norm": 2.2495510578155518,
      "learning_rate": 0.0012727272727272728,
      "loss": 0.038,
      "step": 2400
    },
    {
      "epoch": 110.0,
      "grad_norm": 5.650709629058838,
      "learning_rate": 0.0012666666666666666,
      "loss": 0.0481,
      "step": 2420
    },
    {
      "epoch": 110.9090909090909,
      "grad_norm": 3.7933225631713867,
      "learning_rate": 0.0012606060606060606,
      "loss": 0.0299,
      "step": 2440
    },
    {
      "epoch": 111.81818181818181,
      "grad_norm": 1.3900471925735474,
      "learning_rate": 0.0012545454545454546,
      "loss": 0.0365,
      "step": 2460
    },
    {
      "epoch": 112.72727272727273,
      "grad_norm": 0.46113884449005127,
      "learning_rate": 0.0012484848484848486,
      "loss": 0.0437,
      "step": 2480
    },
    {
      "epoch": 113.63636363636364,
      "grad_norm": 2.509728193283081,
      "learning_rate": 0.0012424242424242424,
      "loss": 0.0629,
      "step": 2500
    },
    {
      "epoch": 114.54545454545455,
      "grad_norm": 0.4908851385116577,
      "learning_rate": 0.0012363636363636364,
      "loss": 0.0517,
      "step": 2520
    },
    {
      "epoch": 115.45454545454545,
      "grad_norm": 2.5812222957611084,
      "learning_rate": 0.0012303030303030303,
      "loss": 0.0427,
      "step": 2540
    },
    {
      "epoch": 116.36363636363636,
      "grad_norm": 1.0894126892089844,
      "learning_rate": 0.0012242424242424241,
      "loss": 0.0262,
      "step": 2560
    },
    {
      "epoch": 117.27272727272727,
      "grad_norm": 3.322981595993042,
      "learning_rate": 0.0012181818181818183,
      "loss": 0.0349,
      "step": 2580
    },
    {
      "epoch": 118.18181818181819,
      "grad_norm": 1.2827073335647583,
      "learning_rate": 0.0012121212121212121,
      "loss": 0.0383,
      "step": 2600
    },
    {
      "epoch": 119.0909090909091,
      "grad_norm": 0.2815510630607605,
      "learning_rate": 0.0012060606060606061,
      "loss": 0.0198,
      "step": 2620
    },
    {
      "epoch": 120.0,
      "grad_norm": 9.744356155395508,
      "learning_rate": 0.0012,
      "loss": 0.0347,
      "step": 2640
    },
    {
      "epoch": 120.9090909090909,
      "grad_norm": 5.705557823181152,
      "learning_rate": 0.0011939393939393939,
      "loss": 0.0522,
      "step": 2660
    },
    {
      "epoch": 121.81818181818181,
      "grad_norm": 2.8304100036621094,
      "learning_rate": 0.001187878787878788,
      "loss": 0.0361,
      "step": 2680
    },
    {
      "epoch": 122.72727272727273,
      "grad_norm": 1.451153039932251,
      "learning_rate": 0.0011818181818181819,
      "loss": 0.0345,
      "step": 2700
    },
    {
      "epoch": 123.63636363636364,
      "grad_norm": 4.1059088706970215,
      "learning_rate": 0.0011757575757575759,
      "loss": 0.0419,
      "step": 2720
    },
    {
      "epoch": 124.54545454545455,
      "grad_norm": 4.533682823181152,
      "learning_rate": 0.0011696969696969697,
      "loss": 0.0309,
      "step": 2740
    },
    {
      "epoch": 125.45454545454545,
      "grad_norm": 3.6719443798065186,
      "learning_rate": 0.0011636363636363637,
      "loss": 0.0253,
      "step": 2760
    },
    {
      "epoch": 126.36363636363636,
      "grad_norm": 1.4874887466430664,
      "learning_rate": 0.0011575757575757574,
      "loss": 0.0163,
      "step": 2780
    },
    {
      "epoch": 127.27272727272727,
      "grad_norm": 2.761281967163086,
      "learning_rate": 0.0011515151515151516,
      "loss": 0.0375,
      "step": 2800
    },
    {
      "epoch": 128.1818181818182,
      "grad_norm": 0.6720583438873291,
      "learning_rate": 0.0011454545454545454,
      "loss": 0.0448,
      "step": 2820
    },
    {
      "epoch": 129.0909090909091,
      "grad_norm": 2.6801912784576416,
      "learning_rate": 0.0011393939393939394,
      "loss": 0.027,
      "step": 2840
    },
    {
      "epoch": 130.0,
      "grad_norm": 9.939739227294922,
      "learning_rate": 0.0011333333333333334,
      "loss": 0.0224,
      "step": 2860
    },
    {
      "epoch": 130.9090909090909,
      "grad_norm": 1.0062806606292725,
      "learning_rate": 0.0011272727272727272,
      "loss": 0.0306,
      "step": 2880
    },
    {
      "epoch": 131.8181818181818,
      "grad_norm": 2.877490520477295,
      "learning_rate": 0.0011212121212121212,
      "loss": 0.0287,
      "step": 2900
    },
    {
      "epoch": 132.72727272727272,
      "grad_norm": 0.5139176249504089,
      "learning_rate": 0.0011151515151515152,
      "loss": 0.0257,
      "step": 2920
    },
    {
      "epoch": 133.63636363636363,
      "grad_norm": 1.2643846273422241,
      "learning_rate": 0.0011090909090909092,
      "loss": 0.0162,
      "step": 2940
    },
    {
      "epoch": 134.54545454545453,
      "grad_norm": 0.2570510506629944,
      "learning_rate": 0.001103030303030303,
      "loss": 0.0236,
      "step": 2960
    },
    {
      "epoch": 135.45454545454547,
      "grad_norm": 2.778488874435425,
      "learning_rate": 0.001096969696969697,
      "loss": 0.0256,
      "step": 2980
    },
    {
      "epoch": 136.36363636363637,
      "grad_norm": 3.662005662918091,
      "learning_rate": 0.001090909090909091,
      "loss": 0.014,
      "step": 3000
    },
    {
      "epoch": 137.27272727272728,
      "grad_norm": 4.94188117980957,
      "learning_rate": 0.001084848484848485,
      "loss": 0.0085,
      "step": 3020
    },
    {
      "epoch": 138.1818181818182,
      "grad_norm": 2.279940366744995,
      "learning_rate": 0.001078787878787879,
      "loss": 0.0113,
      "step": 3040
    },
    {
      "epoch": 139.0909090909091,
      "grad_norm": 0.6794223189353943,
      "learning_rate": 0.0010727272727272727,
      "loss": 0.0428,
      "step": 3060
    },
    {
      "epoch": 140.0,
      "grad_norm": 2.534548282623291,
      "learning_rate": 0.0010666666666666667,
      "loss": 0.018,
      "step": 3080
    },
    {
      "epoch": 140.9090909090909,
      "grad_norm": 7.368445873260498,
      "learning_rate": 0.0010606060606060605,
      "loss": 0.0119,
      "step": 3100
    },
    {
      "epoch": 141.8181818181818,
      "grad_norm": 3.7286455631256104,
      "learning_rate": 0.0010545454545454545,
      "loss": 0.0147,
      "step": 3120
    },
    {
      "epoch": 142.72727272727272,
      "grad_norm": 7.797715187072754,
      "learning_rate": 0.0010484848484848487,
      "loss": 0.0167,
      "step": 3140
    },
    {
      "epoch": 143.63636363636363,
      "grad_norm": 1.9861422777175903,
      "learning_rate": 0.0010424242424242425,
      "loss": 0.0286,
      "step": 3160
    },
    {
      "epoch": 144.54545454545453,
      "grad_norm": 5.145240783691406,
      "learning_rate": 0.0010363636363636365,
      "loss": 0.018,
      "step": 3180
    },
    {
      "epoch": 145.45454545454547,
      "grad_norm": 0.21566365659236908,
      "learning_rate": 0.0010303030303030303,
      "loss": 0.0117,
      "step": 3200
    },
    {
      "epoch": 146.36363636363637,
      "grad_norm": 0.5723376274108887,
      "learning_rate": 0.0010242424242424243,
      "loss": 0.0139,
      "step": 3220
    },
    {
      "epoch": 147.27272727272728,
      "grad_norm": 0.15065008401870728,
      "learning_rate": 0.001018181818181818,
      "loss": 0.0304,
      "step": 3240
    },
    {
      "epoch": 148.1818181818182,
      "grad_norm": 0.09854882210493088,
      "learning_rate": 0.0010121212121212122,
      "loss": 0.0127,
      "step": 3260
    },
    {
      "epoch": 149.0909090909091,
      "grad_norm": 0.11314697563648224,
      "learning_rate": 0.001006060606060606,
      "loss": 0.0166,
      "step": 3280
    },
    {
      "epoch": 150.0,
      "grad_norm": 0.41806936264038086,
      "learning_rate": 0.001,
      "loss": 0.006,
      "step": 3300
    },
    {
      "epoch": 150.9090909090909,
      "grad_norm": 0.7650377750396729,
      "learning_rate": 0.000993939393939394,
      "loss": 0.0066,
      "step": 3320
    },
    {
      "epoch": 151.8181818181818,
      "grad_norm": 0.11565697193145752,
      "learning_rate": 0.000987878787878788,
      "loss": 0.0069,
      "step": 3340
    },
    {
      "epoch": 152.72727272727272,
      "grad_norm": 0.036635346710681915,
      "learning_rate": 0.0009818181818181818,
      "loss": 0.0017,
      "step": 3360
    },
    {
      "epoch": 153.63636363636363,
      "grad_norm": 0.20319390296936035,
      "learning_rate": 0.0009757575757575757,
      "loss": 0.0029,
      "step": 3380
    },
    {
      "epoch": 154.54545454545453,
      "grad_norm": 0.03821872919797897,
      "learning_rate": 0.0009696969696969698,
      "loss": 0.0013,
      "step": 3400
    },
    {
      "epoch": 155.45454545454547,
      "grad_norm": 0.04959877207875252,
      "learning_rate": 0.0009636363636363637,
      "loss": 0.001,
      "step": 3420
    },
    {
      "epoch": 156.36363636363637,
      "grad_norm": 0.03732777759432793,
      "learning_rate": 0.0009575757575757576,
      "loss": 0.0009,
      "step": 3440
    },
    {
      "epoch": 157.27272727272728,
      "grad_norm": 0.029851028695702553,
      "learning_rate": 0.0009515151515151516,
      "loss": 0.0007,
      "step": 3460
    },
    {
      "epoch": 158.1818181818182,
      "grad_norm": 0.028208864852786064,
      "learning_rate": 0.0009454545454545454,
      "loss": 0.0009,
      "step": 3480
    },
    {
      "epoch": 159.0909090909091,
      "grad_norm": 0.03305594623088837,
      "learning_rate": 0.0009393939393939394,
      "loss": 0.0007,
      "step": 3500
    },
    {
      "epoch": 160.0,
      "grad_norm": 0.020218797028064728,
      "learning_rate": 0.0009333333333333333,
      "loss": 0.0007,
      "step": 3520
    },
    {
      "epoch": 160.9090909090909,
      "grad_norm": 0.020272983238101006,
      "learning_rate": 0.0009272727272727273,
      "loss": 0.0006,
      "step": 3540
    },
    {
      "epoch": 161.8181818181818,
      "grad_norm": 0.017066925764083862,
      "learning_rate": 0.0009212121212121213,
      "loss": 0.0007,
      "step": 3560
    },
    {
      "epoch": 162.72727272727272,
      "grad_norm": 0.01983029581606388,
      "learning_rate": 0.0009151515151515152,
      "loss": 0.0006,
      "step": 3580
    },
    {
      "epoch": 163.63636363636363,
      "grad_norm": 0.016038231551647186,
      "learning_rate": 0.0009090909090909091,
      "loss": 0.0005,
      "step": 3600
    },
    {
      "epoch": 164.54545454545453,
      "grad_norm": 0.015464802272617817,
      "learning_rate": 0.0009030303030303031,
      "loss": 0.0006,
      "step": 3620
    },
    {
      "epoch": 165.45454545454547,
      "grad_norm": 0.013303623534739017,
      "learning_rate": 0.000896969696969697,
      "loss": 0.0006,
      "step": 3640
    },
    {
      "epoch": 166.36363636363637,
      "grad_norm": 0.020271888002753258,
      "learning_rate": 0.0008909090909090909,
      "loss": 0.0005,
      "step": 3660
    },
    {
      "epoch": 167.27272727272728,
      "grad_norm": 0.028462087735533714,
      "learning_rate": 0.0008848484848484849,
      "loss": 0.0006,
      "step": 3680
    },
    {
      "epoch": 168.1818181818182,
      "grad_norm": 0.012310737743973732,
      "learning_rate": 0.0008787878787878789,
      "loss": 0.0005,
      "step": 3700
    },
    {
      "epoch": 169.0909090909091,
      "grad_norm": 0.014018095098435879,
      "learning_rate": 0.0008727272727272727,
      "loss": 0.0005,
      "step": 3720
    },
    {
      "epoch": 170.0,
      "grad_norm": 0.03441176190972328,
      "learning_rate": 0.0008666666666666667,
      "loss": 0.0005,
      "step": 3740
    },
    {
      "epoch": 170.9090909090909,
      "grad_norm": 0.015399419702589512,
      "learning_rate": 0.0008606060606060606,
      "loss": 0.0005,
      "step": 3760
    },
    {
      "epoch": 171.8181818181818,
      "grad_norm": 0.02536342851817608,
      "learning_rate": 0.0008545454545454545,
      "loss": 0.0005,
      "step": 3780
    },
    {
      "epoch": 172.72727272727272,
      "grad_norm": 0.022170742973685265,
      "learning_rate": 0.0008484848484848485,
      "loss": 0.0004,
      "step": 3800
    },
    {
      "epoch": 173.63636363636363,
      "grad_norm": 0.01048983633518219,
      "learning_rate": 0.0008424242424242424,
      "loss": 0.0005,
      "step": 3820
    },
    {
      "epoch": 174.54545454545453,
      "grad_norm": 0.012894351966679096,
      "learning_rate": 0.0008363636363636363,
      "loss": 0.0004,
      "step": 3840
    },
    {
      "epoch": 175.45454545454547,
      "grad_norm": 0.01230335421860218,
      "learning_rate": 0.0008303030303030304,
      "loss": 0.0005,
      "step": 3860
    },
    {
      "epoch": 176.36363636363637,
      "grad_norm": 0.014429972507059574,
      "learning_rate": 0.0008242424242424243,
      "loss": 0.0004,
      "step": 3880
    },
    {
      "epoch": 177.27272727272728,
      "grad_norm": 0.019114309921860695,
      "learning_rate": 0.0008181818181818183,
      "loss": 0.0004,
      "step": 3900
    },
    {
      "epoch": 178.1818181818182,
      "grad_norm": 0.011068160645663738,
      "learning_rate": 0.0008121212121212122,
      "loss": 0.0004,
      "step": 3920
    },
    {
      "epoch": 179.0909090909091,
      "grad_norm": 0.014590305276215076,
      "learning_rate": 0.000806060606060606,
      "loss": 0.0004,
      "step": 3940
    },
    {
      "epoch": 180.0,
      "grad_norm": 0.020898064598441124,
      "learning_rate": 0.0008,
      "loss": 0.0004,
      "step": 3960
    },
    {
      "epoch": 180.9090909090909,
      "grad_norm": 0.01487298496067524,
      "learning_rate": 0.0007939393939393939,
      "loss": 0.0004,
      "step": 3980
    },
    {
      "epoch": 181.8181818181818,
      "grad_norm": 0.016182508319616318,
      "learning_rate": 0.0007878787878787878,
      "loss": 0.0004,
      "step": 4000
    },
    {
      "epoch": 182.72727272727272,
      "grad_norm": 0.019855521619319916,
      "learning_rate": 0.0007818181818181819,
      "loss": 0.0004,
      "step": 4020
    },
    {
      "epoch": 183.63636363636363,
      "grad_norm": 0.012928507290780544,
      "learning_rate": 0.0007757575757575758,
      "loss": 0.0004,
      "step": 4040
    },
    {
      "epoch": 184.54545454545453,
      "grad_norm": 0.012302614748477936,
      "learning_rate": 0.0007696969696969697,
      "loss": 0.0003,
      "step": 4060
    },
    {
      "epoch": 185.45454545454547,
      "grad_norm": 0.010755009017884731,
      "learning_rate": 0.0007636363636363637,
      "loss": 0.0004,
      "step": 4080
    },
    {
      "epoch": 186.36363636363637,
      "grad_norm": 0.012195337563753128,
      "learning_rate": 0.0007575757575757576,
      "loss": 0.0003,
      "step": 4100
    },
    {
      "epoch": 187.27272727272728,
      "grad_norm": 0.006746841128915548,
      "learning_rate": 0.0007515151515151515,
      "loss": 0.0003,
      "step": 4120
    },
    {
      "epoch": 188.1818181818182,
      "grad_norm": 0.011578205972909927,
      "learning_rate": 0.0007454545454545455,
      "loss": 0.0004,
      "step": 4140
    },
    {
      "epoch": 189.0909090909091,
      "grad_norm": 0.011952400207519531,
      "learning_rate": 0.0007393939393939393,
      "loss": 0.0004,
      "step": 4160
    },
    {
      "epoch": 190.0,
      "grad_norm": 0.017594382166862488,
      "learning_rate": 0.0007333333333333333,
      "loss": 0.0003,
      "step": 4180
    },
    {
      "epoch": 190.9090909090909,
      "grad_norm": 0.009577020071446896,
      "learning_rate": 0.0007272727272727273,
      "loss": 0.0003,
      "step": 4200
    },
    {
      "epoch": 191.8181818181818,
      "grad_norm": 0.019698135554790497,
      "learning_rate": 0.0007212121212121212,
      "loss": 0.0003,
      "step": 4220
    },
    {
      "epoch": 192.72727272727272,
      "grad_norm": 0.01126286294311285,
      "learning_rate": 0.0007151515151515152,
      "loss": 0.0003,
      "step": 4240
    },
    {
      "epoch": 193.63636363636363,
      "grad_norm": 0.007272339891642332,
      "learning_rate": 0.0007090909090909091,
      "loss": 0.0003,
      "step": 4260
    },
    {
      "epoch": 194.54545454545453,
      "grad_norm": 0.010785342194139957,
      "learning_rate": 0.000703030303030303,
      "loss": 0.0003,
      "step": 4280
    },
    {
      "epoch": 195.45454545454547,
      "grad_norm": 0.009497033432126045,
      "learning_rate": 0.000696969696969697,
      "loss": 0.0003,
      "step": 4300
    },
    {
      "epoch": 196.36363636363637,
      "grad_norm": 0.012631895951926708,
      "learning_rate": 0.0006909090909090909,
      "loss": 0.0003,
      "step": 4320
    },
    {
      "epoch": 197.27272727272728,
      "grad_norm": 0.010300937108695507,
      "learning_rate": 0.0006848484848484849,
      "loss": 0.0003,
      "step": 4340
    },
    {
      "epoch": 198.1818181818182,
      "grad_norm": 0.010501235723495483,
      "learning_rate": 0.0006787878787878789,
      "loss": 0.0003,
      "step": 4360
    },
    {
      "epoch": 199.0909090909091,
      "grad_norm": 0.008290627039968967,
      "learning_rate": 0.0006727272727272728,
      "loss": 0.0003,
      "step": 4380
    },
    {
      "epoch": 200.0,
      "grad_norm": 0.01487993448972702,
      "learning_rate": 0.0006666666666666666,
      "loss": 0.0003,
      "step": 4400
    },
    {
      "epoch": 200.9090909090909,
      "grad_norm": 0.016380617395043373,
      "learning_rate": 0.0006606060606060606,
      "loss": 0.0003,
      "step": 4420
    },
    {
      "epoch": 201.8181818181818,
      "grad_norm": 0.008123660460114479,
      "learning_rate": 0.0006545454545454545,
      "loss": 0.0003,
      "step": 4440
    },
    {
      "epoch": 202.72727272727272,
      "grad_norm": 0.0076727368868887424,
      "learning_rate": 0.0006484848484848484,
      "loss": 0.0003,
      "step": 4460
    },
    {
      "epoch": 203.63636363636363,
      "grad_norm": 0.012753269635140896,
      "learning_rate": 0.0006424242424242425,
      "loss": 0.0003,
      "step": 4480
    },
    {
      "epoch": 204.54545454545453,
      "grad_norm": 0.008979832753539085,
      "learning_rate": 0.0006363636363636364,
      "loss": 0.0003,
      "step": 4500
    }
  ],
  "logging_steps": 20,
  "max_steps": 6600,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 300,
  "save_steps": 500,
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
